{"name":"Spark-arules","tagline":"A Scala and Apache Spark based implementation of Association Rules algorithms.","body":"![Dr.Krusche & Partner PartG](https://raw.github.com/skrusche63/spark-elastic/master/images/dr-kruscheundpartner.png)\r\n\r\n## Top-K (Non Redundant) Association Rules with Spark\r\n\r\nAssociation rule mining is a wide-spread technique to determine hidden interesting relations between items in large-scale \r\ntransaction databases. This technique is often applied to data recorded by point-of-sale systems in supermarkets and is able \r\nto determine associations of the following kind:\r\n\r\n> A customer who is willing to buy one or more products together is likely to also buy other items as well.\r\n\r\nAssociation rules are used as a basis for decision making in promotional pricing, product placement and more. The application of \r\nsuch rules, however, is not restricted to market basket analysis and will be used in intrusion detection, web usage mining and other \r\nareas.\r\n\r\n### Apache Spark\r\n\r\n\r\nFrom the [Apache Spark](https://spark.apache.org/) website:\r\n\r\n> Apache Spark is a fast and general engine for large-scale data processing and is up to 100x faster than Hadoop MR in memory.\r\n\r\nThe increasing number of associated projects, such as [Spark SQL](https://spark.apache.org/sql/) and [Spark Streaming](https://spark.apache.org/streaming/), enables Spark to become the future  Unified Data Insight Platform. With this perspective in mind, in this project we have integrated recently published Association Rule algorithms with Spark. This allows for a seamless usage of association rule mining either with batch or streaming data sources.\r\n\r\n### Top-K (Non Redundant) Association Rules\r\n\r\nFinding interesting associations between items in transaction databases is a fundamental data mining task. Finding association rules is \r\nusually accompanied by the following control parameters:\r\n\r\n* **Support**: The percentage of transactions of the database where the rules occurs.\r\n\r\n* **Confidence**: The support of the rule divided by the support of its *antecedent*.\r\n\r\nThe goal of association rule mining then is to discover all rules that have a support and confidence that is higher to user-defined thresholds \r\n*minimum support* and *minimum confidence*. The challenge is choose the right thresholds with respect to the considered transaction database.\r\n\r\nThis is a major problem, as one usually has limited resources for analyzing the mining results, and fine tuning of the thresholds is time-consuming job. The problem is especially associated with *minimum support*:\r\n\r\n**Threshold is set too high**: \r\nThis generates too few results and valuable information may be omitted.\r\n\r\n**Threshold is set too low**: \r\nThis can generate a huge amount of results, and the mining task may become very slow.\r\n\r\nThe dependency of association rule algorithms on *minimum confidence* makes it almost impossible to \"automate\" association rule mining or use it streaming data sources.\r\n\r\nIn 2012, [Philippe-Fournier Viger](http://www.philippe-fournier-viger.com/) redefined the problem of association mining as **Top-K Association Rule Mining**. The proposed algorithm only depends on the parameters *k*, the number of rules to be generated, and *minimum confidence*. For more information, continue to read [here](http://www.philippe-fournier-viger.com/spmf/top_k_non_redundant_association_rules.pdf).\r\n\r\n\r\nWe adapted Viger's original implementation and made his **Top-K** and **Top-K Non Redundant** algorithms available for Spark.\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}